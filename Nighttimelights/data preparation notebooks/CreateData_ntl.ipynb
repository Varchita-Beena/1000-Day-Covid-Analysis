{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "CreateData_ntl.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "457d1135"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import pandas as pd\n",
        "from pyhdf.SD import SD, SDC\n",
        "import os\n",
        "import math\n",
        "import datetime as dt\n",
        "import h5py"
      ],
      "id": "457d1135",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3024bb89"
      },
      "source": [
        "# Here we create 8-day data for Night Time Light emissions from anthropogenic sources"
      ],
      "id": "3024bb89"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba2cf158"
      },
      "source": [
        "The VNP46A2 product used here is quite useful to study the night time light emissions from human activities. This will help us in understanding the patterns in human activity of all kinds including covid lockdowns and restricions. The VNP46 estimates the light emissions from other sources like lunar (actual moonlight), aerosol and surface reflections. All these estimates are used to provide the atmosphere and lunar corrected data to accurately understand the light emissions from human generated activities. Temporal gap filling is also done to cope with any missing data or gaps."
      ],
      "id": "ba2cf158"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a13ae445"
      },
      "source": [
        "The data is provided in the form of HDF-EOS hdf5 format with the extension of files being h5. We use h5py package to parse and read the data. This is a daily product so each file consists of data pertaining to a day. Also, each file only consists of data of 10degree x 10degree tile of linear latitude/longitude grid. There are 460 such land title on the globe for which daily data is provided under this product. We selected a polygon to filter the data for India which resulted into the selection of 14 10degree x 10 degree tiles. So in total 14 x 1000 (number days of interest under this study), we have roughly 14000 files. "
      ],
      "id": "a13ae445"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e64e9c95"
      },
      "source": [
        "An example file is \"VNP46A2.A2020008.h27v07.001.2021053073636.h5\" which specifies that this file file is for 8th day of year 2020. Also, it has data of the tile which is 28th horizontally and 8th vertically. Essentially it has data of the following 10degree x 10degree cell:\n",
        "- 90 degree to 100 degree longitude\n",
        "- 20 degree to 10 degree latitude "
      ],
      "id": "e64e9c95"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75864222"
      },
      "source": [
        "We use \"Gap_Filled_DNB_BRDF-Corrected_NTL\" scientific dataset which is Gap Filled BRDF corrected DNB NTL as explained above. The size of the dataset in each file is 2400 x 2400. Each cell essentially has value for 500 meters x 500 meters area. Since data for 10 degrees is given in 2400 gridded cells linearly, we Generate data accordingly at three resolutions:\n",
        "- 0.05 degree x 0.05 degree: Roughly an area of 5.6kms x 5.6kms. Here we combine 12 cells on both latitude and longitude dimensions to give us a 0.05 degree x 0.05 degree cell.\n",
        "- 0.25 degree x 0.25 degree: Roughly an area of 33kms x 33kms. Here we combine 60 cells on both latitude and longitude dimensions to give us a 0.25 degree x 0.25 degree cell.\n",
        "- 1 degree x 1 degree: Roughly an area of 110kms x 110kms. Here we combine 240 cells on both latitude and longitude dimensions to give us a 1 degree x 1 degree cell.\n",
        "Each cell under any scheme is denoted by the centers of their range of coordinates.\n",
        "Based on the coordinates available for each city, we map the city to one of the respective cell under each scheme."
      ],
      "id": "75864222"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "650cf191"
      },
      "source": [
        "Under any of the three resolution schemes, whicle aggregating the data, we record different statistical measures like mean, standard deviation, minimum, maximum, median and values at different percentiles like 10,25,75 and 90"
      ],
      "id": "650cf191"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d88f7836"
      },
      "source": [
        "For all the three resolution schemes, we generate data at the following two temporal levels:\n",
        "- 8 days: Each month is assigned four weeks - (1st to 8th, 9th to 16th, 17th to 24th, 25th to end of the month), so that they are standard for all years and can be compared across the years\n",
        "- daily"
      ],
      "id": "d88f7836"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7b9380ad"
      },
      "source": [
        "GRANULES_PATH=\"ntl/VNP46A2/\"\n",
        "OUTPUT_PATH=\"data/\"\n",
        "LOCATION_FILE = 'geonames-all-cities-with-a-population-1000-3.csv'\n",
        "WEEK_MAPPINGS = 'data/week_mappings_ntl.csv'\n",
        "week_data = pd.read_csv(WEEK_MAPPINGS)\n",
        "week_data=week_data.set_index(\"week\")[\"granules\"].to_dict()\n",
        "for week in week_data:\n",
        "    week_data[week]=week_data[week].split(\"#\")"
      ],
      "id": "7b9380ad",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88cc680e"
      },
      "source": [
        "metrics={\"Gap_Filled_DNB_BRDF-Corrected_NTL\":\"/HDFEOS/GRIDS/VNP_Grid_DNB/Data Fields/Gap_Filled_DNB_BRDF-Corrected_NTL\"}"
      ],
      "id": "88cc680e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bd675e67"
      },
      "source": [
        "degree=1"
      ],
      "id": "bd675e67",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6717b9f"
      },
      "source": [
        "fp=open(LOCATION_FILE,\"r\")\n",
        "evrythng=fp.readlines()\n",
        "fp.close()\n",
        "locations=set()\n",
        "for line in evrythng[1:]:\n",
        "    line=line.split(\"\\n\")[0].split(\";\")\n",
        "    [lat,long]=line[-1].split(\",\")\n",
        "    if degree==1:\n",
        "        lat=min(math.ceil(float(lat)),90.0)-0.5\n",
        "        long=min(math.ceil(float(long)),180.0)-0.5\n",
        "    elif degree==0.05:\n",
        "        lat=round(90-(int((90-float(lat))/0.05)*0.05+0.025),3)\n",
        "        long=round(180-(int((180-float(long))/0.05)*0.05+0.025),3)\n",
        "    elif degree==0.25:\n",
        "        lat=round(90-(int((90-float(lat))/0.25)*0.25+0.125),3)\n",
        "        long=round(180-(int((180-float(long))/0.25)*0.25+0.125),3)\n",
        "    else:\n",
        "        print(\"wrong degree resolution provided!\")\n",
        "    locations.add((lat,long))"
      ],
      "id": "f6717b9f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "019b38e2"
      },
      "source": [
        "def get_indices(location,granule):\n",
        "    lat_tile_number=int(granule.split(\".\")[2].split(\"v\")[1])\n",
        "    long_tile_number=int(granule.split(\".\")[2].split(\"v\")[0].split(\"h\")[1])\n",
        "    tile_lat_start_index=lat_tile_number*2400\n",
        "    tile_lat_end_index=tile_lat_start_index+2400-1\n",
        "    tile_long_start_index=long_tile_number*2400\n",
        "    tile_long_end_index=tile_long_start_index+2400-1\n",
        "    (lat,long)=location\n",
        "    if degree==1:\n",
        "        lat_start_index=int((90-lat-0.5)*240)\n",
        "        lat_end_index=lat_start_index+240-1\n",
        "        long_start_index=int((long-0.5+180)*240)\n",
        "        long_end_index=long_start_index+240-1\n",
        "    elif degree==0.05:\n",
        "        lat_start_index=round(((90-lat-0.025)/0.05)*12)\n",
        "        lat_end_index=lat_start_index+12-1\n",
        "        long_start_index=round(((long-0.025+180)/0.05)*12)\n",
        "        long_end_index=long_start_index+12-1\n",
        "    elif degree==0.25:\n",
        "        lat_start_index=round(((90-lat-0.125)/0.25)*60)\n",
        "        lat_end_index=lat_start_index+60-1\n",
        "        long_start_index=round(((long-0.125+180)/0.25)*60)\n",
        "        long_end_index=long_start_index+60-1\n",
        "    else:\n",
        "        print(\"wrong degree resolution provided!\")\n",
        "    \n",
        "    if lat_start_index>=tile_lat_start_index and lat_start_index<tile_lat_end_index:\n",
        "        lat_range=(lat_start_index-tile_lat_start_index,lat_end_index-tile_lat_start_index)\n",
        "    else:\n",
        "        return None\n",
        "    if long_start_index>=tile_long_start_index and long_start_index<tile_long_end_index:\n",
        "        long_range=(long_start_index-tile_long_start_index,long_end_index-tile_long_start_index)\n",
        "    else:\n",
        "        return None\n",
        "    return [lat_range,long_range]"
      ],
      "id": "019b38e2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "40116aab",
        "outputId": "effb2fe7-ed13-4564-d3f7-989f285b9465"
      },
      "source": [
        "OUTPUT_FILENAME=OUTPUT_PATH+\"indian_cities_\"+str(degree)+\"degx\"+str(degree)+\"deg_ntl.csv\"\n",
        "fp=open(OUTPUT_FILENAME,\"w\")\n",
        "fp.write(\",\".join([\"location\",\"week\",\"metric\",\"mean\",\"sdev\",\"min\",\"10p\",\"25p\",\"median\",\"75p\",\"90p\",\"max\"])+\"\\n\")\n",
        "fp.close()\n",
        "for week in week_data:\n",
        "    print(week)\n",
        "    #t1=dt.datetime.now()\n",
        "    week_metric_data={}\n",
        "    for metric in metrics:\n",
        "        week_metric_data[metric]={}\n",
        "    for granule in week_data[week]:\n",
        "        f=h5py.File(GRANULES_PATH+granule, \"r\")\n",
        "        for metric in metrics:\n",
        "            metric_data=f[metrics[metric]]\n",
        "            for location in locations:\n",
        "                indices=get_indices(location,granule)\n",
        "                if indices is None:\n",
        "                    continue\n",
        "                [lat_range,long_range]=indices\n",
        "                temp=metric_data[lat_range[0]:lat_range[1]+1,long_range[0]:long_range[1]+1]\n",
        "                temp=temp[np.where((temp>=0) & (temp<65535))]\n",
        "                if len(temp)<=0:\n",
        "                    continue\n",
        "                if location not in week_metric_data[metric]:\n",
        "                    week_metric_data[metric][location]=[]\n",
        "                week_metric_data[metric][location]=np.append(week_metric_data[metric][location],temp)\n",
        "    fp=open(OUTPUT_FILENAME,\"a\")\n",
        "    for metric in week_metric_data:\n",
        "        for location in week_metric_data[metric]:\n",
        "            avg=round(np.mean(week_metric_data[metric][location]))\n",
        "            sdev=round(np.std(week_metric_data[metric][location]))\n",
        "            levels=[round(np.percentile(week_metric_data[metric][location],th)) for th in [0,10,25,50,75,90,100]]\n",
        "            fp.write(\",\".join([str(location[0])+\"#\"+str(location[1]),week,metric]+[str(i) for i in [avg,sdev]+levels])+\"\\n\")\n",
        "    fp.close()\n",
        "    #t2=dt.datetime.now()\n",
        "    #print((t2-t1).seconds)"
      ],
      "id": "40116aab",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.2019.week1\n",
            "1.2019.week2\n",
            "1.2019.week3\n",
            "1.2019.week4\n",
            "2.2019.week1\n",
            "2.2019.week2\n",
            "2.2019.week3\n",
            "2.2019.week4\n",
            "3.2019.week1\n",
            "3.2019.week2\n",
            "3.2019.week3\n",
            "3.2019.week4\n",
            "4.2019.week1\n",
            "4.2019.week2\n",
            "4.2019.week3\n",
            "4.2019.week4\n",
            "5.2019.week1\n",
            "5.2019.week2\n",
            "5.2019.week3\n",
            "5.2019.week4\n",
            "6.2019.week1\n",
            "6.2019.week2\n",
            "6.2019.week3\n",
            "6.2019.week4\n",
            "7.2019.week1\n",
            "7.2019.week2\n",
            "7.2019.week3\n",
            "7.2019.week4\n",
            "8.2019.week1\n",
            "8.2019.week2\n",
            "8.2019.week3\n",
            "8.2019.week4\n",
            "9.2019.week1\n",
            "9.2019.week2\n",
            "9.2019.week3\n",
            "9.2019.week4\n",
            "10.2019.week1\n",
            "10.2019.week2\n",
            "10.2019.week3\n",
            "10.2019.week4\n",
            "11.2019.week1\n",
            "11.2019.week2\n",
            "11.2019.week3\n",
            "11.2019.week4\n",
            "12.2019.week1\n",
            "12.2019.week2\n",
            "12.2019.week3\n",
            "12.2019.week4\n",
            "1.2020.week1\n",
            "1.2020.week2\n",
            "1.2020.week3\n",
            "1.2020.week4\n",
            "2.2020.week1\n",
            "2.2020.week2\n",
            "2.2020.week3\n",
            "2.2020.week4\n",
            "3.2020.week1\n",
            "3.2020.week2\n",
            "3.2020.week3\n",
            "3.2020.week4\n",
            "4.2020.week1\n",
            "4.2020.week2\n",
            "4.2020.week3\n",
            "4.2020.week4\n",
            "5.2020.week1\n",
            "5.2020.week2\n",
            "5.2020.week3\n",
            "5.2020.week4\n",
            "6.2020.week1\n",
            "6.2020.week2\n",
            "6.2020.week3\n",
            "6.2020.week4\n",
            "7.2020.week1\n",
            "7.2020.week2\n",
            "7.2020.week3\n",
            "7.2020.week4\n",
            "8.2020.week1\n",
            "8.2020.week2\n",
            "8.2020.week3\n",
            "8.2020.week4\n",
            "9.2020.week1\n",
            "9.2020.week2\n",
            "9.2020.week3\n",
            "9.2020.week4\n",
            "10.2020.week1\n",
            "10.2020.week2\n",
            "10.2020.week3\n",
            "10.2020.week4\n",
            "11.2020.week1\n",
            "11.2020.week2\n",
            "11.2020.week3\n",
            "11.2020.week4\n",
            "12.2020.week1\n",
            "12.2020.week2\n",
            "12.2020.week3\n",
            "12.2020.week4\n",
            "1.2021.week1\n",
            "1.2021.week2\n",
            "1.2021.week3\n",
            "1.2021.week4\n",
            "2.2021.week1\n",
            "2.2021.week2\n",
            "2.2021.week3\n",
            "2.2021.week4\n",
            "3.2021.week1\n",
            "3.2021.week2\n",
            "3.2021.week3\n",
            "3.2021.week4\n",
            "4.2021.week1\n",
            "4.2021.week2\n",
            "4.2021.week3\n",
            "4.2021.week4\n",
            "5.2021.week1\n",
            "5.2021.week2\n",
            "5.2021.week3\n",
            "5.2021.week4\n",
            "6.2021.week1\n",
            "6.2021.week2\n",
            "6.2021.week3\n",
            "6.2021.week4\n",
            "7.2021.week1\n",
            "7.2021.week2\n",
            "7.2021.week3\n",
            "7.2021.week4\n",
            "8.2021.week1\n",
            "8.2021.week2\n",
            "8.2021.week3\n",
            "8.2021.week4\n",
            "9.2021.week1\n",
            "9.2021.week2\n",
            "9.2021.week3\n",
            "9.2021.week4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f832fd86"
      },
      "source": [
        ""
      ],
      "id": "f832fd86",
      "execution_count": null,
      "outputs": []
    }
  ]
}