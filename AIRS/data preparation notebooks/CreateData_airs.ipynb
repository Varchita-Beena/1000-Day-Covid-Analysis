{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "CreateData_airs.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8Swv9URGd5o"
      },
      "source": [
        "# Dataset \n",
        "\n",
        "Launched in 2002, the Atmospheric Infrared Sounder (AIRS) instrument aboard NASA’s Aqua satellite provides 3D measurements of temperature, water vapor, trace gases, and surface and cloud properties through the atmospheric column.\n",
        "<br>\n",
        "<br>\n",
        "AIRS is a system that measures the abundances of trace elements in the atmosphere, such as CO. Data is accessible daily (AIRS3STD), weekly (AIRS3ST8), or monthly (AIRS3ST8) (AIRS3STM). The sensor monitors the quantity of CO in the atmosphere's whole vertical column profile (from the surface to the top of the atmosphere). The data is saved in HDF format.<br>\n",
        "\n",
        "<br>\n",
        "The data products can be found at [AIRS data from Earthdata Search](https://search.earthdata.nasa.gov/search?q=AIRS3&fi=AIRS&ac=true&fst0=Atmosphere&fsm0=Air%20Quality&fs10=Carbon%20Monoxide)\n",
        "<br>\n",
        "\n",
        "## Data Processing Levels<br>\n",
        "The data products of NASA's Earth Observing System Data and Information System (EOSDIS) are processed at four different levels, from Level 0 to Level 4. Raw data at full instrument resolution is used to create Level 0 products. Data is transformed into more relevant parameters and formats at higher levels.\n",
        "There are several levels of data presentation. (Level 0, Level 1A, Level 1B, Level 1C, Level 2, Level 2A, Level 2B, Level 3, Level 3A, Level 4). Level 3: Variables mapped on uniform space-time grid scales, usually with some completeness and consistency. Level 3A: L3A data are generally periodic summaries (weekly, ten-day, monthly) of L2 products.\n",
        "<br><br>\n",
        "## About AIRS\n",
        "The product used here is AIRS Version 6 (V6) Level 3(L3). Geophysical parameters have been averaged into latitude/longitude grid cells in the AIRS L3 files. Grid maps include longitude coordinates ranging from -180.0° to +180.0° and latitude coordinates ranging from -90.0° to +90.0°.\n",
        "<br><br>\n",
        "## 1°x1° L3 products\n",
        "The V6 Level 2 (L2) swath products are used to create these L3 gridded goods.\n",
        "Which of the L2 goods are merged to form these L3 products is determined by the L2 quality indicators. In general, L2 retrieved quantities with \"best\" (=0) or \"good\" (=1) quality indicators are included in the sums that build L3 gridded products. Geophysical and quality metrics have been averaged and binned into 1°x1° grid cells in these L3 files. There are equivalent maps of standard deviation, counts, minimum, maximum, and, in some circumstances, error estimate for each grid map of mean values. The counts map provides the user with the number of points per bin that were included in the statistics  Values of -9999 or a count of 0 indicate invalid or missing data.<br><br>\n",
        "The L3 standard products are intended for use by the general public in their research and comprise recovered parameters on standard pressure levels that roughly match instrument vertical resolution. Temperature and water vapour profiles are presented at pressure levels of 24 (TempPresLvls) or 12 (H2OPresLvls).<br><br>\n",
        "The ascending and descending portions of the orbit are separated in these L3 products, where \"ascending or descending\" refers to the direction of movement of the sub-satellite point in the satellite track. Movement is climbing from the Southern Hemisphere to the Northern Hemisphere, with an equatorial crossing time of 1:30 PM local time; movement is descending from the Northern Hemisphere to the Southern Hemisphere, with an equatorial crossing time of 1:30 AM local time. Outside of the polar zones, these correspond to daytime and nighttime, respectively.These AIRS L3 products have daily, 8-day (half of the 16-day Aqua orbit repeat cycle), and monthly temporal resolution (calendar).<br><br>\n",
        "## L3 Standard Product Temporal Characteristics.\n",
        "Daily: “Complex” data, leaves in gores between satellite tracks (missing), 1°x1° spatial resolution, 1-day temporal resolution.<br>\n",
        "8-Day: “Moderate” data, no gores, and some data dropouts, 1°x1° spatial resolution, 8-day temporal resolution based on Aqua 16-day repeat cycle.\n",
        "Monthly: “Simple” data, no gores, complete coverage, 1°x1° spatial resolution, Monthly (calendar)<br><br>\n",
        "There will be gores (cells with no data) between the satellite trajectories where there is no coverage for that day in the daily L3 products. The 8-day and monthly L3 products will most likely have comprehensive worldwide coverage without gores, with missing data mainly in regions where the retrieval technique proved problematic or when topography intrudes into the profile's lower altitude regime.<br><br>\n",
        "**Here the data downloaded and used is AIRS3ST8 (Standard L3, AIRS only and 8-day) from dates 01-01-2019 to 30-9-2021and is of 54GB** \n"
      ],
      "id": "U8Swv9URGd5o"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7afd3444"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import pandas as pd\n",
        "from pyhdf.SD import SD, SDC\n",
        "import os\n",
        "import math"
      ],
      "id": "7afd3444",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9b7b5c9f"
      },
      "source": [
        "GRANULES_PATH=''#path where hdf files of AIRS3ST8 is stored.\n",
        "LOCATION_FILE = ''#path of files with details of cities with latitude, longitude, population, elevation etc (geonames-all-cities-with-a-population-1000-3.csv).\n",
        "WEEK_MAPPINGS = 'data/week_mappings_airs.csv' #the week mappings file made from GenerateWeekMappings_airs.ipynb. This tells us about which granule falls under which week.\n",
        "OUTPUT_PATH=\"data/\" #where you want to store the output file\n",
        "week_data = pd.read_csv(WEEK_MAPPINGS)\n",
        "week_data=week_data.set_index(\"week\")[\"granule\"].to_dict()"
      ],
      "id": "9b7b5c9f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cBzjuSTuzOm"
      },
      "source": [
        "The following metrics to be taken for analysis."
      ],
      "id": "7cBzjuSTuzOm"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62ab591d"
      },
      "source": [
        "metrics=['SurfPres_Forecast_TqJ_A', 'SurfSkinTemp_TqJ_A', 'EmisIR_TqJ_A', 'Temperature_TqJ_A', 'SurfAirTemp_TqJ_A', 'TropPres_TqJ_A', 'TropTemp_TqJ_A', 'TotH2OVap_TqJ_A', 'H2O_MMR_Lyr_TqJ_A', 'H2O_MMR_TqJ_A', 'H2O_MMR_Surf_TqJ_A', 'RelHum_TqJ_A', 'RelHumSurf_TqJ_A', 'RelHum_liquid_TqJ_A', 'RelHumSurf_liquid_TqJ_A', 'TropHeight_TqJ_A', 'GPHeight_TqJ_A', 'CloudFrc_TqJ_A', 'CloudTopPres_TqJ_A', 'CloudTopTemp_TqJ_A', 'FineCloudFrc_TqJ_A', 'CoarseCloudFrc_TqJ_A', 'CoarseCloudPres_TqJ_A', 'CoarseCloudTemp_TqJ_A', 'TotO3_TqJ_A', 'O3_VMR_TqJ_A', 'CO_VMR_TqJ_A', 'CH4_VMR_TqJ_A', 'OLR_TqJ_A', 'ClrOLR_TqJ_A', 'SurfPres_Forecast_TqJ_D', 'SurfSkinTemp_TqJ_D', 'EmisIR_TqJ_D', 'Temperature_TqJ_D', 'SurfAirTemp_TqJ_D', 'TropPres_TqJ_D', 'TropTemp_TqJ_D', 'TotH2OVap_TqJ_D', 'H2O_MMR_Lyr_TqJ_D', 'H2O_MMR_TqJ_D', 'H2O_MMR_Surf_TqJ_D', 'RelHum_TqJ_D', 'RelHumSurf_TqJ_D', 'RelHum_liquid_TqJ_D', 'RelHumSurf_liquid_TqJ_D', 'TropHeight_TqJ_D', 'GPHeight_TqJ_D', 'CloudFrc_TqJ_D', 'CloudTopPres_TqJ_D', 'CloudTopTemp_TqJ_D', 'FineCloudFrc_TqJ_D', 'CoarseCloudFrc_TqJ_D', 'CoarseCloudPres_TqJ_D', 'CoarseCloudTemp_TqJ_D', 'TotO3_TqJ_D', 'O3_VMR_TqJ_D', 'CO_VMR_TqJ_D', 'CH4_VMR_TqJ_D', 'OLR_TqJ_D', 'ClrOLR_TqJ_D']"
      ],
      "id": "62ab591d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dMAHfQqU9Pg"
      },
      "source": [
        "# Providing grid names to granules in terms of city from LOCATION_FILE.<br>\n",
        "Horizontal resolution= 1°x1° degree (360x180)<br>\n",
        "Upper Left Point= -180.0, 90.0<br>\n",
        "Lower Right Point= 180.0, -90.0<br>\n",
        "The latitudes and longitudes of the grid box centers are provided in the data\n",
        "(LATITUDE, LONGITUDE). The upper left box center location is (89.5, -179.5)\n",
        "and the lower right box center location is (-89.5, +179.5). The spatial extent of the 1x1 degree grid spans the upper left (+90, -180) to lower right (-90,\n",
        "+180). The longitude value goes from -180 to 180 and latitude value goes from 90 to -90.<br>\n",
        "<br>\n",
        "The grid's center is given as (latitude, longitude) in data. To map that to out city (latitude, longitude) from LOCATION_FILE do:<br>\n",
        "1. change latitude and longitude into float<br>\n",
        "2. Take the ceiling value using math library.<br>\n",
        "3. Subtract 0.5 value from both latitude and longitude.<br>\n",
        "4. To not cross the maximum value 90 for latitude and 180 for longitude, always take minimum between ceil latitude value and max latitude (180) and minimum between ceil value and max longitude value (90). <br>\n",
        "<br>\n",
        "Say, we have (18.2, 22.7) to map. According to (lat, long) center values provided in data, this longitude value is in the grid [18, 19] and latitude value is in the grid [22, 23]. So the center should come as (lat, long) -> (18.5, 22.5). Therefore by using ceiling the value will be (18.2, 22.7) -> (19, 23). Then by subtracting 0.5, will have (19, 23) - > (18.5, 22.5).<br>\n",
        "\n",
        "\n",
        "\n"
      ],
      "id": "7dMAHfQqU9Pg"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5a6c7cf9"
      },
      "source": [
        "fp=open(LOCATION_FILE,\"r\")\n",
        "evrythng=fp.readlines()\n",
        "fp.close()\n",
        "locations=set()\n",
        "for line in evrythng[1:]:\n",
        "    line=line.split(\"\\n\")[0].split(\";\")\n",
        "    [lat,long]=line[-1].split(\",\")\n",
        "    lat=min(math.ceil(float(lat)),90.0)-0.5\n",
        "    long=min(math.ceil(float(long)),180.0)-0.5\n",
        "    locations.add((lat,long))"
      ],
      "id": "5a6c7cf9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18CXUvyLdKxe"
      },
      "source": [
        "# L3 Standard Product Example File Names\n",
        "8-Day Product Dec 3-10, 2009 processed using only AIRS radiances:<br>\n",
        "Name: AIRS.2009.12.03.L3.RetStd_IR008.v6.0.9.0.G2002123120634.hdf<br>\n",
        "Shortname: AIRS3ST8\n",
        "\n",
        "# HDF File<br>\n",
        "HDF file contains sub datasets of every metrics. To access the metrics of particular latitude and longitude, those latitudes and logitudes are also stored as datasets.<br><br>\n",
        "We have coordinates of 1 degree x 1 degree cities. In total in India 1 deg x 1 deg unique cities are 310. Now we want metrics for these locations (for these latitudes and longitudes).<br><br>\n",
        "How to get (latitude, longitude) pair from data for out (latitude, longitude) pair (of cities data)?. The latitude is stored as 2D matrix and also the longitude. We will get lat_obj by selecting latitude and long_obj by selecting longitude. <br><br>\n",
        "\n",
        "Total latitude blocks (latitude indices) will be 180 (90 to -90) and total longitude blocks (longitude indices) will be 360 (-180 to 180). The matrices for both latitude and longitude is 180x360. Run for loop over latitude range and longitude range, will go through all indices and get latitude and longitude, compare them with our reqiured latitude and longitude values. If matched store them in dictionary.<br><br>\n",
        "\n",
        "Let say we want O3 metric at latitude x and longitude y (i.e. O3[x,y]).Run over all indices of latitude and longitude, extract latitude value from latitude matrix for latitude and longitude matrix. Same way extract longitude value from longitude matrix. Compare values with x, y. If matched then store them in location_indices dictionary with key as (x,y) and value as (latitude index, longitude index). Now any metric can be extracted with these location indices.<br><br>\n",
        "\n",
        "Each grid includes data for the entire globe in 360 x 180 grid cells each 1 x 1 degree of latitude/longitude. Most fields appear in the 4 main grids: ascending, descending, ascending_TqJoint, and descending_TqJoint. The ascending grids collect data taken while the spacecraft is in the ascending part of its orbit. This is generally daytime, except near the poles.<br><br>\n",
        "\n",
        "The separation into ascending and descending portions of the orbit mitigates the\n",
        "suppression of the diurnal signal in the data. Ascending field names have '_A'\n",
        "appended. Descending field names have '_D' appended. Each grid provides a 360x180xn array of standard retrieval mean (without any appendix), standard deviation (with an appendix of _sdev), minimum (with an appendix of _min), maximum (with an appendix of _max), input count (with an appendix of _ct). Some also contain standard error (with an appendix of _err). The 'extra dimension' n=24 for temperature and n=12 for water vapor and n=1 if the product is not a profile.<br><br>\n",
        "So for example in the ascending grid the main (mean) Temperature field is\n",
        "“Temperature_A” and it has ancillary fields “Temperature_A_ct”,\n",
        "“Temperature_A_sdev”, “Temperature_A_min”, “Temperature_A_max”, and\n",
        "“Temperature_A_err”.<br><br>\n",
        "When values are greater than 1 for a metric, they are stored using '#'.<br><br>\n",
        "# Preparing CSV file<br>\n",
        "Csv file will have the following fields: location, week, metric, mean, sdev, min, max. This data will be made for 1 degree x 1 degree city. <br><br>At the end all the values read above are stored under respective columns in a csv file."
      ],
      "id": "18CXUvyLdKxe"
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ddec4dec",
        "outputId": "f9b5a442-c6ce-4bc5-bb02-0e66d6a382cc"
      },
      "source": [
        "OUTPUT_FILENAME=OUTPUT_PATH+\"indian_cities_1degx1deg_airs.csv\"\n",
        "fp=open(OUTPUT_FILENAME,\"w\")\n",
        "fp.write(\",\".join([\"location\",\"week\",\"metric\",\"mean\",\"sdev\",\"min\",\"max\"])+\"\\n\")\n",
        "fp.close()\n",
        "for week in week_data:\n",
        "    print(week)\n",
        "    data=[]\n",
        "    granule=week_data[week]\n",
        "    hdf = SD(GRANULES_PATH+granule, SDC.READ)\n",
        "    location_indices={}\n",
        "    lat_obj=hdf.select(\"Latitude\")\n",
        "    lat_data=lat_obj.get()\n",
        "    long_obj=hdf.select(\"Longitude\")\n",
        "    long_data=long_obj.get()\n",
        "    for lat_index in range(180):\n",
        "        for long_index in range(360):\n",
        "            lat=lat_data[lat_index,long_index]\n",
        "            long=long_data[lat_index,long_index]\n",
        "            if (lat,long) in locations:\n",
        "                location_indices[(lat,long)]=(lat_index,long_index)\n",
        "    for metric in metrics:\n",
        "        metric_obj=hdf.select(metric)\n",
        "        metric_data=metric_obj.get()\n",
        "        metric_shape=metric_data.shape\n",
        "        \n",
        "        metric_std_obj=hdf.select(metric+\"_sdev\")\n",
        "        metric_std_data=metric_std_obj.get()\n",
        "        \n",
        "        metric_min_obj=hdf.select(metric+\"_min\")\n",
        "        metric_min_data=metric_min_obj.get()\n",
        "        \n",
        "        metric_max_obj=hdf.select(metric+\"_max\")\n",
        "        metric_max_data=metric_max_obj.get()\n",
        "        for location in location_indices:\n",
        "            (lat_index,long_index)=location_indices[location]\n",
        "            if len(metric_shape)==2:\n",
        "                data.append([\"#\".join([str(i) for i in location]),week,metric,metric_data[lat_index,long_index],metric_std_data[lat_index,long_index],metric_min_data[lat_index,long_index],metric_max_data[lat_index,long_index]])\n",
        "            elif len(metric_shape)==3:\n",
        "                for i in range(metric_shape[0]):\n",
        "                    data.append([\"#\".join([str(i) for i in location]),week,metric+\"#\"+str(i+1),metric_data[i,lat_index,long_index],metric_std_data[i,lat_index,long_index],metric_min_data[i,lat_index,long_index],metric_max_data[i,lat_index,long_index]])\n",
        "            else:\n",
        "                print(metric,\"metric_shape<2 or metric_shape>3\")\n",
        "    fp=open(OUTPUT_FILENAME,\"a\")\n",
        "    for vector in data:\n",
        "        fp.write(\",\".join([str(i) for i in vector])+\"\\n\")\n",
        "    fp.close()"
      ],
      "id": "ddec4dec",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.2019.week1\n",
            "1.2019.week2\n",
            "1.2019.week3\n",
            "1.2019.week4\n",
            "2.2019.week1\n",
            "2.2019.week2\n",
            "2.2019.week3\n",
            "3.2019.week1\n",
            "3.2019.week2\n",
            "3.2019.week3\n",
            "3.2019.week4\n",
            "4.2019.week1\n",
            "4.2019.week2\n",
            "4.2019.week3\n",
            "4.2019.week4\n",
            "5.2019.week1\n",
            "5.2019.week2\n",
            "5.2019.week3\n",
            "5.2019.week4\n",
            "6.2019.week1\n",
            "6.2019.week2\n",
            "6.2019.week3\n",
            "7.2019.week1\n",
            "7.2019.week2\n",
            "7.2019.week3\n",
            "7.2019.week4\n",
            "8.2019.week1\n",
            "8.2019.week2\n",
            "8.2019.week3\n",
            "8.2019.week4\n",
            "9.2019.week1\n",
            "9.2019.week2\n",
            "9.2019.week3\n",
            "9.2019.week4\n",
            "10.2019.week1\n",
            "10.2019.week2\n",
            "10.2019.week3\n",
            "10.2019.week4\n",
            "11.2019.week1\n",
            "11.2019.week2\n",
            "11.2019.week3\n",
            "11.2019.week4\n",
            "12.2019.week1\n",
            "12.2019.week2\n",
            "12.2019.week3\n",
            "1.2020.week1\n",
            "1.2020.week2\n",
            "1.2020.week3\n",
            "1.2020.week4\n",
            "2.2020.week1\n",
            "2.2020.week2\n",
            "2.2020.week3\n",
            "2.2020.week4\n",
            "3.2020.week1\n",
            "3.2020.week2\n",
            "3.2020.week3\n",
            "3.2020.week4\n",
            "4.2020.week1\n",
            "4.2020.week2\n",
            "4.2020.week3\n",
            "4.2020.week4\n",
            "5.2020.week1\n",
            "5.2020.week2\n",
            "5.2020.week3\n",
            "6.2020.week1\n",
            "6.2020.week2\n",
            "6.2020.week3\n",
            "6.2020.week4\n",
            "7.2020.week1\n",
            "7.2020.week2\n",
            "7.2020.week3\n",
            "7.2020.week4\n",
            "8.2020.week1\n",
            "8.2020.week2\n",
            "9.2020.week1\n",
            "9.2020.week2\n",
            "9.2020.week3\n",
            "9.2020.week4\n",
            "10.2020.week1\n",
            "10.2020.week2\n",
            "10.2020.week3\n",
            "10.2020.week4\n",
            "11.2020.week1\n",
            "11.2020.week2\n",
            "11.2020.week3\n",
            "12.2020.week1\n",
            "12.2020.week2\n",
            "12.2020.week3\n",
            "12.2020.week4\n",
            "1.2021.week1\n",
            "1.2021.week2\n",
            "1.2021.week3\n",
            "1.2021.week4\n",
            "2.2021.week1\n",
            "2.2021.week2\n",
            "2.2021.week3\n",
            "2.2021.week4\n",
            "3.2021.week1\n",
            "3.2021.week2\n",
            "3.2021.week3\n",
            "4.2021.week2\n",
            "4.2021.week3\n",
            "4.2021.week4\n",
            "5.2021.week1\n",
            "5.2021.week2\n",
            "5.2021.week3\n",
            "5.2021.week4\n",
            "6.2021.week1\n",
            "6.2021.week2\n",
            "6.2021.week3\n",
            "7.2021.week1\n",
            "7.2021.week2\n",
            "7.2021.week3\n",
            "7.2021.week4\n",
            "8.2021.week1\n",
            "8.2021.week2\n",
            "8.2021.week3\n",
            "8.2021.week4\n",
            "9.2021.week1\n",
            "9.2021.week2\n",
            "9.2021.week3\n",
            "10.2021.week1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38ed96e1"
      },
      "source": [
        ""
      ],
      "id": "38ed96e1",
      "execution_count": null,
      "outputs": []
    }
  ]
}